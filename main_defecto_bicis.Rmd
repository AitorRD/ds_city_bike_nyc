---
title: "Proyecto Data Science - City Bike Dataset"
author: 
  - "Costela Guijosa, Jose Luis"
  - "Reyes López, Marta"
  - "Rodríguez Dueñas, Aitor"
  - "Sánchez Jiménez, Manuel"
date: "Febrero de 2025"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("dplyr")) install.packages("dplyr")
if (!require("readr")) install.packages("readr")
if (!require("geosphere")) install.packages("geosphere")
if (!require("lubridate")) install.packages("lubridate")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("plotly")) install.packages("plotly")
if (!require("patchwork")) install.packages("patchwork")

library(patchwork)
library(dplyr)
library(readr)
library(geosphere)
library(lubridate)
library(ggplot2)
library(plotly)

```

### ¿Como puedo saber cuáles podrían ser las bicis más susceptibles a tener algún defecto?

En esta sección, consideramos el escenario en el que somos propietarios de una empresa de bicicletas. Resulta fundamental implementar un plan de mantenimiento eficiente. Para ello, es necesario llevar a cabo un análisis que permita al equipo de mantenimiento identificar qué bicicletas presentan un mayor riesgo de fallos o cuáles deberían recibir prioridad para un mantenimiento preventivo, con el objetivo de minimizar posibles incidentes e imprevistos.

El dataset disponible nos permite realizar este análisis mediante la evaluación de distintos factores, como el promedio de velocidad de las bicicletas, que podría indicar la presencia de anomalías mecánicas, o la frecuencia de uso de cada unidad, lo que ayudaría a detectar aquellas que requieren una revisión más exhaustiva.

## Resolución de preguntas

El primer aspecto a considerar es el preprocesamiento necesario para garantizar un análisis preciso y relevante. Para garantizar que el análisis refleje el estado actual del servicio, se ha decidido utilizar únicamente los datos del último año. Esta decisión se debe a que la información más antigua podría no ser relevante para la toma de decisiones operativas. Por ejemplo, conocer la velocidad de una bicicleta en 2016 no aportaría valor, ya que, en caso de haber presentado algún problema, es probable que ya haya sido reparada o retirada del sistema. Al centrarnos en los datos recientes, aseguramos que los hallazgos sean representativos de la situación actual y permitan tomar decisiones basadas en información actualizada.

```{r bicis_supceptibles_fallo_filtrado}

df_susceptibles_fallo <- df_preprocesado %>%
  filter(format(starttime, "%Y") %in% c("2020", "2021"))

```

El siguiente aspecto a considerar en el preprocesamiento es la eliminación de valores atípicos (outliers). En particular, es importante identificar y gestionar casos en los que la bicicleta haya sido tomada y dejada en la misma estación. Dado el método de cálculo utilizado para la velocidad (distancia entre estaciones dividida por el tiempo de viaje), estos casos resultan en una velocidad igual a cero. Este fenómeno puede distorsionar el análisis y afectar la interpretación de los resultados, por lo que es necesario aplicar estrategias adecuadas para su tratamiento.

```{r test}

df_susceptibles_fallo <- df_susceptibles_fallo %>% filter(distance_km != 0)

```

Una vez eliminados los registros con distancias iguales a cero, se procede a revisar la existencia de posibles valores anómalos en las variables de velocidad y duración del viaje. Esta revisión tiene como objetivo detectar y eliminar las siguientes casuísticas:
- Viajes con velocidades irreales, superiores a las que puede alcanzar un ciclista en condiciones normales.
- Viajes en los que, en lugar de desplazarse de un punto a otro, el usuario ha utilizado la bicicleta para realizar un recorrido circular o recreativo. Este tipo de trayectos puede generar velocidades calculadas que no reflejan un uso típico del sistema, afectando al análisis posterior.

```{r test}

# Boxplot de la velocidad de los viajes
p1_speed <- ggplot(df_susceptibles_fallo, aes(y = speed)) +
  geom_boxplot(fill="steelblue", outlier.color="red") +
  labs(title="Boxplot de la velocidad de los Viajes", y="Velocidad") +
  theme_minimal()

# Boxplot de la duracion de los viajes
p2_distance <- ggplot(df_susceptibles_fallo, aes(y = tripduration)) +
  geom_boxplot(fill="steelblue", outlier.color="red") +
  labs(title="Boxplot de la duracion de los Viajes", y="Distancia") +
  theme_minimal()
  
(p1_speed | p2_distance)

# Q1 <- quantile(df_susceptibles_fallo$speed, 0.25, na.rm = TRUE)
# Q3 <- quantile(df_susceptibles_fallo$speed, 0.75, na.rm = TRUE)
# IQR <- Q3 - Q1
# 
# limite_inferior <- Q1 - 1.5 * IQR
# limite_superior <- Q3 + 1.5 * IQR
# 
# df_sin_outliers <- df_susceptibles_fallo %>%
#   filter(speed >= limite_inferior & speed <= limite_superior)
# 
# df_outliers <- df_susceptibles_fallo %>%
#   filter(speed < limite_inferior | speed > limite_superior)
# 
# nrow(df_outliers)
# 
# # Boxplot de la velocidad de los viajes
# ggplot(df_sin_outliers, aes(y = speed)) +
#   geom_boxplot(fill="steelblue", outlier.color="red") +
#   labs(title="Boxplot de la velocidad de los viajes", y="Velocidad") +
#   theme_minimal()

```

Notas:
- Comentar que cogemos el último año ya que para resolver la pregunta no nos interesan datos de hace mucho tiempo
- Revisar si quitar el +1 del group by porque es algo que me chirria
- Ver si quitar outliers tipo velocidad 0 en aquellos viajes de coger y dejar la bici en el mismo lado
- Revisar si quitar outliers de bicis que están mucho tiempo usandose (podría ser que son gente que cogen la bici durante un día entero sin soltarla)
- Al cambiar los datos cambiarán seguramente el número de clusters -> Actualizar estos valores
- Hacer estudio de que clusters son más prioritarios que otros para darles un valor de prioridad.
- Ordenar bicis respecto al uso que tienen que tener
- Hacer estudio de bicis que no se usen desde hace mucho? --> Prioridad mega alta porque tienen que tener algún fallo gordo
- Revisar listado de bicis que se hayan usado en años anteriores pero no en este? O descartar esas suponiendo que es que han salido de uso.

```{r bicis_supceptibles_fallo}

df_filtrado <- df %>%
  filter(format(starttime, "%Y") %in% c("2020", "2021"))

# crear el df de bicicletas group by bike id, creando las columnas:
# - Velocidad media
# - Última vez utilizada
# - Primera vez utilizada
# - Número de km recorridos totales
# - Distancia media recorrida por día
# - ¿Cuantas veces se ha utilizado?
# - Alguna más?


df_bikes <- df_filtrado %>%
  group_by(bikeid) %>%
  summarise(
    velocidad_media = mean(speed, na.rm = TRUE),
    ultima_vez_utilizada = max(starttime, na.rm = TRUE),
    primera_vez_utilizada = min(starttime, na.rm = TRUE),
    km_totales = sum(distance_km, na.rm = TRUE),
    distancia_media_por_dia = mean(distance_km / as.numeric(difftime(max(starttime), min(starttime), units="days") + 1), na.rm = TRUE), 
    veces_utilizada = n(),
    dias_uso = as.numeric(difftime(ultima_vez_utilizada, primera_vez_utilizada, units="days")) + 1,
    media_usos_por_dia = veces_utilizada / dias_uso 
  
  )

# TEEEEST

datos_cluster <- df_bikes %>% 
  select(velocidad_media, distancia_media_por_dia, media_usos_por_dia)

# Normalizar los datos para evitar sesgos por diferencias de escala
datos_cluster <- scale(datos_cluster)

# Determinar el número óptimo de clusters usando el método del codo
set.seed(123)  # Para reproducibilidad
wss <- sapply(1:10, function(k){
  kmeans(datos_cluster, centers = k, nstart = 10)$tot.withinss
})

# Graficar el método del codo
plot(1:10, wss, type="b", pch = 19, frame = FALSE,
     xlab="Número de Clusters", ylab="Suma de cuadrados dentro del grupo")

# Aplicar k-means con el número óptimo de clusters (ejemplo con k=3)
set.seed(123)
kmeans_result <- kmeans(datos_cluster, centers = 5, nstart = 10)

# Agregar la etiqueta de cluster al dataframe original
df_bikes$cluster <- as.factor(kmeans_result$cluster)

plot_ly(df_bikes, 
        x = ~velocidad_media, 
        y = ~distancia_media_por_dia, 
        z = ~media_usos_por_dia, 
        color = ~cluster, 
        colors = "Set1",
        type = "scatter3d", 
        mode = "markers") %>%
  layout(title = "Clustering de Bicicletas en 3D",
         scene = list(xaxis = list(title = "Velocidad Media"),
                      yaxis = list(title = "Distancia Media por Día"),
                      zaxis = list(title = "Media Usos por Día")))

```

Siguiendo rúbrica: - La complejidad e idoneidad de las técnicas de resolución empleadas. Se puntuarán mejor el uso de técnicas de machine learning o tests estadísticos a simples visualizaciones directas de los datos.

Siguiendo rúbrica para visualización: - ¿Ha mostrado alguna visualización? ¿Cuántas? - ¿Siguen los criterios explicados en clase respecto de la veracidad y claridad de las visualizaciones? - ¿Les falta algún tipo de información contextual importante para entender el gráfico? - ¿Es el gráfico más adecuado para lo que quieren representar?

## Conclusiones

Indicar resultados y conclusiones a las preguntas.

Siguiendo rúbrica: - ¿Han respondido a las cuestiones planteadas? - ¿Es coherente la conclusión con el resto del proceso? - ¿Es realmente una conclusión lo que se ha obtenido o es una declaración de intenciones?
